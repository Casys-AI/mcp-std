{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# 01 - The Problem: Why Casys MCP Gateway Exists\n\nBefore learning how the gateway works, let's **experience the problems it solves**.\n\n## Learning Objectives\n\nAfter this notebook, you will:\n\n- [ ] See how tool schemas consume context tokens\n- [ ] Measure the latency cost of sequential execution\n- [ ] Understand why these problems limit MCP adoption\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Problem 1: The Context Explosion\n",
    "\n",
    "Every MCP tool has a **schema** that describes:\n",
    "\n",
    "- What the tool does (description)\n",
    "- What inputs it needs (parameters)\n",
    "- What it returns (output schema)\n",
    "\n",
    "When you connect an MCP server, **ALL schemas are loaded** into the LLM context.\n",
    "\n",
    "Let's simulate what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP Server Context Consumption\n",
      "==================================================\n",
      "\n",
      "github       15 tools √ó 800 tokens = 12,000 tokens (6.0%)\n",
      "filesystem    8 tools √ó 600 tokens =  4,800 tokens (2.4%)\n",
      "database     12 tools √ó 900 tokens = 10,800 tokens (5.4%)\n",
      "playwright   20 tools √ó 750 tokens = 15,000 tokens (7.5%)\n",
      "slack        10 tools √ó 700 tokens =  7,000 tokens (3.5%)\n",
      "notion       14 tools √ó 850 tokens = 11,900 tokens (5.9%)\n",
      "jira         16 tools √ó 820 tokens = 13,120 tokens (6.6%)\n",
      "serena       25 tools √ó 650 tokens = 16,250 tokens (8.1%)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "TOTAL:       120 tools                   90,870 tokens (45.4%)\n",
      "\n",
      "‚ö†Ô∏è  45.4% of your context window is consumed BEFORE you even start talking!\n"
     ]
    }
   ],
   "source": [
    "// Simulate MCP tool schemas (realistic sizes)\n",
    "const mcpServers = {\n",
    "  github: {\n",
    "    tools: 15,\n",
    "    avgTokensPerTool: 800,\n",
    "    examples: [\"create_issue\", \"list_commits\", \"create_pr\", \"search_code\"],\n",
    "  },\n",
    "  filesystem: {\n",
    "    tools: 8,\n",
    "    avgTokensPerTool: 600,\n",
    "    examples: [\"read_file\", \"write_file\", \"list_directory\", \"search_files\"],\n",
    "  },\n",
    "  database: {\n",
    "    tools: 12,\n",
    "    avgTokensPerTool: 900,\n",
    "    examples: [\"query\", \"insert\", \"update\", \"create_table\"],\n",
    "  },\n",
    "  playwright: {\n",
    "    tools: 20,\n",
    "    avgTokensPerTool: 750,\n",
    "    examples: [\"navigate\", \"click\", \"screenshot\", \"fill_form\"],\n",
    "  },\n",
    "  slack: {\n",
    "    tools: 10,\n",
    "    avgTokensPerTool: 700,\n",
    "    examples: [\"send_message\", \"search_messages\", \"list_channels\"],\n",
    "  },\n",
    "  notion: {\n",
    "    tools: 14,\n",
    "    avgTokensPerTool: 850,\n",
    "    examples: [\"create_page\", \"query_database\", \"update_block\"],\n",
    "  },\n",
    "  jira: {\n",
    "    tools: 16,\n",
    "    avgTokensPerTool: 820,\n",
    "    examples: [\"create_issue\", \"search_issues\", \"update_status\"],\n",
    "  },\n",
    "  serena: {\n",
    "    tools: 25,\n",
    "    avgTokensPerTool: 650,\n",
    "    examples: [\"analyze_code\", \"find_references\", \"rename_symbol\"],\n",
    "  },\n",
    "};\n",
    "\n",
    "// Calculate total context consumption\n",
    "const CONTEXT_WINDOW = 200_000; // Claude's context window\n",
    "\n",
    "let totalTokens = 0;\n",
    "let totalTools = 0;\n",
    "\n",
    "console.log(\"MCP Server Context Consumption\\n\" + \"=\".repeat(50));\n",
    "console.log();\n",
    "\n",
    "for (const [server, data] of Object.entries(mcpServers)) {\n",
    "  const serverTokens = data.tools * data.avgTokensPerTool;\n",
    "  totalTokens += serverTokens;\n",
    "  totalTools += data.tools;\n",
    "\n",
    "  const percentage = ((serverTokens / CONTEXT_WINDOW) * 100).toFixed(1);\n",
    "  console.log(\n",
    "    `${server.padEnd(12)} ${\n",
    "      data.tools.toString().padStart(2)\n",
    "    } tools √ó ${data.avgTokensPerTool} tokens = ${\n",
    "      serverTokens.toLocaleString().padStart(6)\n",
    "    } tokens (${percentage}%)`,\n",
    "  );\n",
    "}\n",
    "\n",
    "console.log(\"‚îÄ\".repeat(50));\n",
    "const totalPercentage = ((totalTokens / CONTEXT_WINDOW) * 100).toFixed(1);\n",
    "console.log(\n",
    "  `TOTAL:       ${totalTools} tools                   ${\n",
    "    totalTokens.toLocaleString().padStart(6)\n",
    "  } tokens (${totalPercentage}%)`,\n",
    ");\n",
    "console.log();\n",
    "console.log(\n",
    "  `‚ö†Ô∏è  ${totalPercentage}% of your context window is consumed BEFORE you even start talking!`,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### The Impact\n",
    "\n",
    "With ~45% of context consumed by tool schemas:\n",
    "\n",
    "- **Long conversations get truncated** - Important context gets dropped\n",
    "- **Complex tasks fail** - Not enough room for reasoning\n",
    "- **You self-limit** - \"I'll disable that MCP server to save context\"\n",
    "\n",
    "And here's the irony: **You only use 3-5 tools per request**, but you're paying for 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "// The waste calculation - now with visual metrics\nimport { progressBar, compareMetrics } from \"../lib/metrics.ts\";\n\nconst TOOLS_USED_PER_REQUEST = 4; // Average tools actually used\n// Use dynamic totalTools from previous cell instead of hardcoding\nconst tokensUsed = TOOLS_USED_PER_REQUEST * (totalTokens / totalTools);\n\nconst usageRate = (TOOLS_USED_PER_REQUEST / totalTools * 100).toFixed(1);\nconst wastedTokens = totalTokens * (1 - TOOLS_USED_PER_REQUEST / totalTools);\nconst wastedPercentage = (wastedTokens / CONTEXT_WINDOW * 100).toFixed(1);\n\nconsole.log(\"The Waste Problem\\n\" + \"=\".repeat(50));\nconsole.log();\n\n// Visual progress bar showing context consumption\nconsole.log(\"Context Window Usage:\");\nconsole.log(progressBar(totalTokens, CONTEXT_WINDOW, \"consumed by tool schemas\"));\nconsole.log();\n\n// Comparison table: loaded vs used\nconsole.log(\"üìä Token Efficiency Analysis:\\n\");\nconsole.log(compareMetrics(\n  { \"Tokens\": totalTokens, \"Tools\": totalTools },\n  { \"Tokens\": Math.round(tokensUsed), \"Tools\": TOOLS_USED_PER_REQUEST },\n  { labels: { before: \"Loaded\", after: \"Actually Used\" } }\n));\nconsole.log();\n\nconsole.log(`üí∏ You're paying for ${totalTools - TOOLS_USED_PER_REQUEST} tools you don't use!`);\nconsole.log(`üìâ ${wastedPercentage}% of your context window is wasted!`);"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2: Sequential Latency\n",
    "\n",
    "Now let's look at the second problem: **every tool call is sequential**.\n",
    "\n",
    "Consider a simple workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "// Simulate a multi-tool workflow\nimport { displayMermaid } from \"../lib/viz.ts\";\n\ninterface Task {\n  name: string;\n  duration: number; // ms\n  dependsOn: string[];\n}\n\nconst workflow: Task[] = [\n  { name: \"1. Read config file\", duration: 200, dependsOn: [] },\n  { name: \"2. Parse JSON\", duration: 50, dependsOn: [\"1. Read config file\"] },\n  { name: \"3. Fetch GitHub issues\", duration: 800, dependsOn: [\"2. Parse JSON\"] },\n  { name: \"4. Search Slack messages\", duration: 600, dependsOn: [\"2. Parse JSON\"] },\n  {\n    name: \"5. Create Jira ticket\",\n    duration: 400,\n    dependsOn: [\"3. Fetch GitHub issues\", \"4. Search Slack messages\"],\n  },\n];\n\nconsole.log(\"Workflow: Create Jira ticket from config + GitHub + Slack\\n\");\nconsole.log(\"Note: Steps 3 and 4 are INDEPENDENT - they could run in parallel!\");\nconsole.log();\n\n// Visualize the DAG structure with Mermaid\nconst dagDefinition = `graph LR\n    A[\"1. Read config<br/>200ms\"] --> B[\"2. Parse JSON<br/>50ms\"]\n    B --> C[\"3. Fetch GitHub<br/>800ms\"]\n    B --> D[\"4. Search Slack<br/>600ms\"]\n    C --> E[\"5. Create Jira<br/>400ms\"]\n    D --> E\n    \n    style C fill:#9cf,stroke:#333\n    style D fill:#9cf,stroke:#333`;\n\nawait displayMermaid(dagDefinition);"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Execution (Current MCP)\n",
      "=============================================\n",
      "1. Read config file       200ms (total: 200ms)\n",
      "2. Parse JSON             50ms (total: 250ms)\n",
      "3. Fetch GitHub issues    800ms (total: 1050ms)\n",
      "4. Search Slack messages  600ms (total: 1650ms)\n",
      "5. Create Jira ticket     400ms (total: 2050ms)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Total time: 2050ms\n"
     ]
    }
   ],
   "source": [
    "// Sequential execution (how MCP works today)\n",
    "async function executeSequentially(tasks: Task[]): Promise<number> {\n",
    "  let totalTime = 0;\n",
    "\n",
    "  console.log(\"Sequential Execution (Current MCP)\\n\" + \"=\".repeat(45));\n",
    "\n",
    "  for (const task of tasks) {\n",
    "    const start = Date.now();\n",
    "    await new Promise((r) => setTimeout(r, task.duration));\n",
    "    totalTime += task.duration;\n",
    "    console.log(`${task.name.padEnd(25)} ${task.duration}ms (total: ${totalTime}ms)`);\n",
    "  }\n",
    "\n",
    "  return totalTime;\n",
    "}\n",
    "\n",
    "const sequentialTime = await executeSequentially(workflow);\n",
    "console.log(\"‚îÄ\".repeat(45));\n",
    "console.log(`Total time: ${sequentialTime}ms`);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "// Parallel execution (how it COULD work with DAG)\nimport { speedupChart } from \"../lib/metrics.ts\";\n\nasync function executeWithDAG(tasks: Task[]): Promise<number> {\n  const completed = new Map<string, number>();\n  let currentTime = 0;\n\n  console.log(\"\\nParallel Execution (With DAG)\\n\" + \"=\".repeat(45));\n\n  while (completed.size < tasks.length) {\n    // Find tasks that can run now (all dependencies satisfied)\n    const ready = tasks.filter((t) =>\n      !completed.has(t.name) &&\n      t.dependsOn.every((dep) => completed.has(dep))\n    );\n\n    if (ready.length === 0) break;\n\n    // Execute ready tasks in parallel\n    const startTime = currentTime;\n    const maxDuration = Math.max(...ready.map((t) => t.duration));\n\n    if (ready.length > 1) {\n      console.log(`[PARALLEL] Running ${ready.length} tasks simultaneously:`);\n    }\n\n    for (const task of ready) {\n      const endTime = startTime + task.duration;\n      completed.set(task.name, endTime);\n      const prefix = ready.length > 1 ? \"  ‚îú‚îÄ \" : \"\";\n      console.log(`${prefix}${task.name.padEnd(25)} ${task.duration}ms`);\n    }\n\n    currentTime = startTime + maxDuration;\n\n    if (ready.length > 1) {\n      console.log(`  ‚îî‚îÄ Layer complete at ${currentTime}ms`);\n    }\n  }\n\n  return currentTime;\n}\n\nconst parallelTime = await executeWithDAG(workflow);\nconsole.log(\"‚îÄ\".repeat(45));\nconsole.log(`Total time: ${parallelTime}ms`);\nconsole.log();\n\n// Visual speedup comparison chart\nconsole.log(\"üìä Performance Comparison:\\n\");\nconsole.log(speedupChart(sequentialTime, parallelTime));"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Real Cost\n",
    "\n",
    "Let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "// Calculate speedup for summary\nconst speedup = (sequentialTime / parallelTime).toFixed(1);\n\nconsole.log(\"The MCP Scaling Problem - Summary\\n\" + \"=\".repeat(50));\nconsole.log();\nconsole.log(\"üìä CONTEXT WASTE\");\nconsole.log(`   ‚Ä¢ ${totalTools} tools loaded, ${TOOLS_USED_PER_REQUEST} used per request`);\nconsole.log(`   ‚Ä¢ ${totalPercentage}% of context consumed before you start`);\nconsole.log(`   ‚Ä¢ Result: Shorter conversations, truncated responses`);\nconsole.log();\nconsole.log(\"‚è±Ô∏è  LATENCY COST\");\nconsole.log(`   ‚Ä¢ Sequential: ${sequentialTime}ms`);\nconsole.log(`   ‚Ä¢ Could be:   ${parallelTime}ms (${speedup}x faster)`);\nconsole.log(`   ‚Ä¢ Result: Slow workflows, broken flow state`);\nconsole.log();\nconsole.log(\"üöß THE CONSEQUENCE\");\nconsole.log(\"   ‚Ä¢ Power users limit themselves to 7-8 MCP servers\");\nconsole.log(\"   ‚Ä¢ Complex cross-MCP workflows are impractical\");\nconsole.log(\"   ‚Ä¢ The MCP ecosystem can't reach its potential\");\nconsole.log();\nconsole.log(\"‚îÄ\".repeat(50));\nconsole.log(\"üí° This is why Casys MCP Gateway exists.\");"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "---\n\n## Summary\n\nYou've now experienced the two core problems that limit MCP adoption:\n\n1. **Context Explosion** - Tool schemas consume ~45% of context before you start\n2. **Sequential Latency** - Independent tasks wait for each other unnecessarily\n\nThese problems compound as you add more MCP servers, creating a ceiling on what's practical.\n\n**Next:** [02-context-optimization.ipynb](./02-context-optimization.ipynb) - Learn how vector search solves the context problem"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}