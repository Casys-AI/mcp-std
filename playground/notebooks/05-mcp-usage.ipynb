{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Gateway with LLM Integration\n",
    "\n",
    "Demonstrates how LLMs use MCP tools to execute code safely.\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- Start HTTP MCP server\n",
    "- LLM calls tools via MCP\n",
    "- Workflow orchestration with tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install & Start MCP Gateway\n",
    "\n",
    "**First time setup (~2-3 minutes):**\n",
    "\n",
    "- Installs npm dependencies (30-60s)\n",
    "- Downloads BGE-M3 model (60-90s) - 2.2GB\n",
    "- Initializes database & GraphRAG engine\n",
    "\n",
    "**Subsequent runs:** Server starts in ~5 seconds (model is cached)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Casys PML Gateway...\n",
      "\n",
      "üì¶ First time? This will:\n",
      "   1. Install npm dependencies (~30-60s)\n",
      "   2. Download BGE-M3 model (~60-90s, 2.2GB)\n",
      "   3. Initialize database & GraphRAG\n",
      "\n",
      "‚è≥ Please wait, this may take 2-3 minutes on first run...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Start the real MCP Gateway server\n",
    "console.log(\"üöÄ Starting Casys PML Gateway...\");\n",
    "console.log(\"\");\n",
    "console.log(\"üì¶ First time? This will:\");\n",
    "console.log(\"   1. Install npm dependencies (~30-60s)\");\n",
    "console.log(\"   2. Download BGE-M3 model (~60-90s, 2.2GB)\");\n",
    "console.log(\"   3. Initialize database & GraphRAG\");\n",
    "console.log(\"\");\n",
    "console.log(\"‚è≥ Please wait, this may take 2-3 minutes on first run...\");\n",
    "console.log(\"\");\n",
    "\n",
    "const server = new Deno.Command(\"deno\", {\n",
    "  args: [\"run\", \"--allow-all\", \"../examples/server.ts\"],\n",
    "  stdout: \"piped\",\n",
    "  stderr: \"piped\",\n",
    "});\n",
    "\n",
    "const serverProcess = server.spawn();\n",
    "\n",
    "// Read server output to show progress\n",
    "const decoder = new TextDecoder();\n",
    "let ready = false;\n",
    "\n",
    "// Stream output with timeout\n",
    "const timeoutMs = 180000; // 3 minutes\n",
    "const startTime = Date.now();\n",
    "\n",
    "while (!ready && (Date.now() - startTime) < timeoutMs) {\n",
    "  const buf = new Uint8Array(1024);\n",
    "  try {\n",
    "    const n = await serverProcess.stdout.read(buf);\n",
    "    if (n) {\n",
    "      const output = decoder.decode(buf.subarray(0, n));\n",
    "      console.log(output);\n",
    "      if (output.includes(\"‚úÖ MCP Gateway ready!\")) {\n",
    "        ready = true;\n",
    "      }\n",
    "    }\n",
    "  } catch (e) {\n",
    "    // Continue on read errors\n",
    "  }\n",
    "  await new Promise((r) => setTimeout(r, 100));\n",
    "}\n",
    "\n",
    "if (!ready) {\n",
    "  console.log(\"\");\n",
    "  console.log(\"‚ö†Ô∏è  Server taking longer than expected. Checking health...\");\n",
    "}\n",
    "\n",
    "// Verify server is responding\n",
    "await new Promise((r) => setTimeout(r, 2000));\n",
    "const health = await fetch(\"http://localhost:3000/health\");\n",
    "const healthData = await health.json();\n",
    "\n",
    "console.log(\"\");\n",
    "console.log(\"‚úÖ MCP Gateway is ready!\");\n",
    "console.log(`   Status: ${healthData.status}`);\n",
    "console.log(`   Port: 3000`);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import visualization helpers\n",
    "import { dagToMermaid, displayDag } from \"../lib/viz.ts\";\n",
    "\n",
    "console.log(\"üìä DAG visualization loaded (with display functions)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: List Available MCP Tools\n",
    "\n",
    "Query the MCP server using JSON-RPC 2.0 protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// MCP uses JSON-RPC 2.0 protocol\n",
    "const response = await fetch(\"http://localhost:3000/message\", {\n",
    "  method: \"POST\",\n",
    "  headers: { \"Content-Type\": \"application/json\" },\n",
    "  body: JSON.stringify({\n",
    "    jsonrpc: \"2.0\",\n",
    "    id: 1,\n",
    "    method: \"tools/list\",\n",
    "    params: {},\n",
    "  }),\n",
    "});\n",
    "\n",
    "const { result } = await response.json();\n",
    "const { tools } = result;\n",
    "\n",
    "console.log(`üìã Available MCP tools (${tools.length}):\\n`);\n",
    "for (const tool of tools) {\n",
    "  console.log(`üîß ${tool.name}`);\n",
    "  console.log(`   ${tool.description.slice(0, 80)}...`);\n",
    "  console.log();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Call Tool Directly\n",
    "\n",
    "Test tool execution without LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Call MCP tool using JSON-RPC\n",
    "const response = await fetch(\"http://localhost:3000/message\", {\n",
    "  method: \"POST\",\n",
    "  headers: { \"Content-Type\": \"application/json\" },\n",
    "  body: JSON.stringify({\n",
    "    jsonrpc: \"2.0\",\n",
    "    id: 2,\n",
    "    method: \"tools/call\",\n",
    "    params: {\n",
    "      name: \"pml__pml_execute_code\",\n",
    "      arguments: {\n",
    "        code: \"return Array.from({length: 10}, (_, i) => i * i)\",\n",
    "      },\n",
    "    },\n",
    "  }),\n",
    "});\n",
    "\n",
    "const { result } = await response.json();\n",
    "console.log(\"‚úÖ Execution result:\");\n",
    "console.log(JSON.stringify(result, null, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LLM with Tool Calling\n",
    "\n",
    "Now let the LLM decide when to use tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import LLM provider\n",
    "import { createLLM, generateCompletion } from \"../examples/llm-provider.ts\";\n",
    "import { generateText, tool } from \"npm:ai\";\n",
    "import { z } from \"npm:zod\";\n",
    "\n",
    "const apiKey = Deno.env.get(\"ANTHROPIC_API_KEY\") ||\n",
    "  Deno.env.get(\"OPENAI_API_KEY\") ||\n",
    "  Deno.env.get(\"GOOGLE_API_KEY\");\n",
    "\n",
    "if (!apiKey) {\n",
    "  throw new Error(\"Set an API key in llm-demo.ipynb first!\");\n",
    "}\n",
    "\n",
    "const model = createLLM({ apiKey });\n",
    "\n",
    "console.log(\"‚úÖ LLM ready\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the tool for the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const executeCodeTool = tool({\n",
    "  description: \"Execute TypeScript/JavaScript code safely via MCP\",\n",
    "  parameters: z.object({\n",
    "    code: z.string().describe(\"TypeScript/JavaScript code to execute\"),\n",
    "    context: z.any().optional().describe(\"Optional context data\"),\n",
    "  }),\n",
    "  execute: async ({ code, context }) => {\n",
    "    const response = await fetch(\"http://localhost:3000/message\", {\n",
    "      method: \"POST\",\n",
    "      headers: { \"Content-Type\": \"application/json\" },\n",
    "      body: JSON.stringify({\n",
    "        jsonrpc: \"2.0\",\n",
    "        id: Math.random(),\n",
    "        method: \"tools/call\",\n",
    "        params: {\n",
    "          name: \"pml__pml_execute_code\",\n",
    "          arguments: { code, context },\n",
    "        },\n",
    "      }),\n",
    "    });\n",
    "    const { result } = await response.json();\n",
    "    return result.content[0].text;\n",
    "  },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask LLM to Solve Task with Code\n",
    "\n",
    "The LLM will generate and execute code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const result = await generateText({\n",
    "  model,\n",
    "  tools: { execute_code: executeCodeTool },\n",
    "  maxSteps: 5,\n",
    "  prompt:\n",
    "    \"Calculate the sum of all prime numbers between 1 and 100. Write and execute TypeScript code to do this.\",\n",
    "});\n",
    "\n",
    "console.log(\"\\nü§ñ LLM Response:\");\n",
    "console.log(result.text);\n",
    "\n",
    "console.log(\"\\nüìä Tool Calls:\");\n",
    "for (const step of result.steps) {\n",
    "  if (step.toolCalls) {\n",
    "    for (const call of step.toolCalls) {\n",
    "      console.log(`\\nüîß Called: ${call.toolName}`);\n",
    "      console.log(\"   Args:\", JSON.stringify(call.args, null, 2));\n",
    "      console.log(\"   Result:\", JSON.stringify(step.toolResults, null, 2));\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Multi-Step Workflow\n",
    "\n",
    "LLM orchestrates multiple tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The workflow above could be represented as a DAG\n",
    "const workflowDAG = {\n",
    "  id: \"sales-analysis\",\n",
    "  tasks: [\n",
    "    { id: \"parse_data\", tool_name: \"execute_code\", dependencies: [] },\n",
    "    { id: \"calc_revenue\", tool_name: \"execute_code\", dependencies: [\"parse_data\"] },\n",
    "    { id: \"find_best_seller\", tool_name: \"execute_code\", dependencies: [\"parse_data\"] },\n",
    "    { id: \"calc_avg_price\", tool_name: \"execute_code\", dependencies: [\"parse_data\"] },\n",
    "    {\n",
    "      id: \"generate_report\",\n",
    "      tool_name: \"execute_code\",\n",
    "      dependencies: [\"calc_revenue\", \"find_best_seller\", \"calc_avg_price\"],\n",
    "    },\n",
    "  ],\n",
    "};\n",
    "\n",
    "console.log(\"üìä Sales Analysis Workflow as DAG:\");\n",
    "console.log(\"üí° Steps 2-4 could run in parallel!\\n\");\n",
    "\n",
    "// Display visual diagram\n",
    "await displayDag(workflowDAG);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow as DAG\n",
    "\n",
    "The multi-step workflow can be represented as a DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const workflow = await generateText({\n",
    "  model,\n",
    "  tools: { execute_code: executeCodeTool },\n",
    "  maxSteps: 10,\n",
    "  prompt: `Analyze this sales data step by step:\n",
    "1. Calculate total revenue\n",
    "2. Find best selling product\n",
    "3. Calculate average price\n",
    "\n",
    "Data: [{\"product\":\"Laptop\",\"price\":1200,\"qty\":2},{\"product\":\"Mouse\",\"price\":25,\"qty\":5},{\"product\":\"Keyboard\",\"price\":80,\"qty\":3}]\n",
    "\n",
    "Write code to perform each calculation.`,\n",
    "});\n",
    "\n",
    "console.log(\"\\nü§ñ Workflow Result:\");\n",
    "console.log(workflow.text);\n",
    "\n",
    "console.log(\"\\nüìà Steps executed:\", workflow.steps.length);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup\n",
    "\n",
    "Stop the MCP server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverProcess.kill(\"SIGTERM\");\n",
    "await serverProcess.status;\n",
    "console.log(\"‚úÖ Server stopped\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we demonstrated:**\n",
    "\n",
    "- ‚úÖ MCP server exposes tools via HTTP\n",
    "- ‚úÖ LLM discovers and calls tools automatically\n",
    "- ‚úÖ Safe code execution in sandbox\n",
    "- ‚úÖ Multi-step workflow orchestration\n",
    "- ‚úÖ LLM decides when and how to use tools\n",
    "\n",
    "**The MCP Pattern:**\n",
    "\n",
    "```\n",
    "User Query ‚Üí LLM ‚Üí Tool Selection ‚Üí MCP Server ‚Üí Sandbox ‚Üí Results ‚Üí LLM ‚Üí User\n",
    "```\n",
    "\n",
    "This is how Claude Code, Cline, and other AI coding tools work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
