{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# 02 - Context Optimization: From 45% to <5%\n",
    "\n",
    "In the previous notebook, we saw how tool schemas consume 30-50% of context.\n",
    "\n",
    "Now let's solve it with **semantic vector search** and **on-demand loading**.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After this notebook, you will:\n",
    "\n",
    "- [ ] Understand how vector embeddings enable semantic search\n",
    "- [ ] See on-demand tool loading in action\n",
    "- [ ] Measure the context savings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Idea: Load Only What You Need\n",
    "\n",
    "Instead of loading all 120 tools upfront, what if we could:\n",
    "\n",
    "1. **Index** all tools once (at startup)\n",
    "2. **Search** for relevant tools when needed (semantic similarity)\n",
    "3. **Load** only the top 3-5 matching tools\n",
    "\n",
    "```\n",
    "User: \"Read the config file and create a GitHub issue\"\n",
    "     â†“\n",
    "Vector Search: Find tools matching this intent\n",
    "     â†“\n",
    "Results: [filesystem:read_file, github:create_issue, json:parse]\n",
    "     â†“\n",
    "Load: Only these 3 tool schemas (not all 120)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## How Vector Search Works\n\n### Step 1: Create Embeddings\n\nEach tool description is converted to a **vector** (array of numbers) that captures its meaning:\n\n```\n\"read_file: Read contents of a file from the filesystem\"\n    â†“ BGE-M3 model\n[0.023, -0.156, 0.891, ..., 0.044]  (1024 dimensions)\n```\n\n### Step 2: Index in Database\n\nAll embeddings are stored in PGlite with pgvector for fast similarity search.\n\n### Step 3: Query\n\nWhen you ask a question, we:\n\n1. Convert your question to an embedding\n2. Find the closest tool embeddings (cosine similarity)\n3. Return the top-k matches"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 13 tools (sample from 120 total)\n",
      "\n",
      "Tools by server:\n",
      "  github: 3 tools\n",
      "  filesystem: 3 tools\n",
      "  database: 2 tools\n",
      "  slack: 2 tools\n",
      "  playwright: 2 tools\n",
      "  utils: 1 tools\n"
     ]
    }
   ],
   "source": [
    "// Simulate vector search behavior\n",
    "// (In production, this uses real embeddings from BGE-Large-EN)\n",
    "\n",
    "interface Tool {\n",
    "  id: string;\n",
    "  server: string;\n",
    "  name: string;\n",
    "  description: string;\n",
    "  tokens: number;\n",
    "}\n",
    "\n",
    "// Sample of our 120 tools\n",
    "const allTools: Tool[] = [\n",
    "  {\n",
    "    id: \"gh-1\",\n",
    "    server: \"github\",\n",
    "    name: \"create_issue\",\n",
    "    description: \"Create a new issue in a GitHub repository\",\n",
    "    tokens: 850,\n",
    "  },\n",
    "  {\n",
    "    id: \"gh-2\",\n",
    "    server: \"github\",\n",
    "    name: \"list_commits\",\n",
    "    description: \"List commits from a repository branch\",\n",
    "    tokens: 720,\n",
    "  },\n",
    "  {\n",
    "    id: \"gh-3\",\n",
    "    server: \"github\",\n",
    "    name: \"create_pr\",\n",
    "    description: \"Create a pull request\",\n",
    "    tokens: 920,\n",
    "  },\n",
    "  {\n",
    "    id: \"fs-1\",\n",
    "    server: \"filesystem\",\n",
    "    name: \"read_file\",\n",
    "    description: \"Read contents of a file from the filesystem\",\n",
    "    tokens: 480,\n",
    "  },\n",
    "  {\n",
    "    id: \"fs-2\",\n",
    "    server: \"filesystem\",\n",
    "    name: \"write_file\",\n",
    "    description: \"Write content to a file\",\n",
    "    tokens: 520,\n",
    "  },\n",
    "  {\n",
    "    id: \"fs-3\",\n",
    "    server: \"filesystem\",\n",
    "    name: \"list_directory\",\n",
    "    description: \"List files and folders in a directory\",\n",
    "    tokens: 450,\n",
    "  },\n",
    "  {\n",
    "    id: \"db-1\",\n",
    "    server: \"database\",\n",
    "    name: \"query\",\n",
    "    description: \"Execute a SQL query on the database\",\n",
    "    tokens: 680,\n",
    "  },\n",
    "  {\n",
    "    id: \"db-2\",\n",
    "    server: \"database\",\n",
    "    name: \"insert\",\n",
    "    description: \"Insert rows into a database table\",\n",
    "    tokens: 750,\n",
    "  },\n",
    "  {\n",
    "    id: \"sl-1\",\n",
    "    server: \"slack\",\n",
    "    name: \"send_message\",\n",
    "    description: \"Send a message to a Slack channel\",\n",
    "    tokens: 620,\n",
    "  },\n",
    "  {\n",
    "    id: \"sl-2\",\n",
    "    server: \"slack\",\n",
    "    name: \"search_messages\",\n",
    "    description: \"Search for messages in Slack\",\n",
    "    tokens: 580,\n",
    "  },\n",
    "  {\n",
    "    id: \"pw-1\",\n",
    "    server: \"playwright\",\n",
    "    name: \"screenshot\",\n",
    "    description: \"Take a screenshot of a webpage\",\n",
    "    tokens: 540,\n",
    "  },\n",
    "  {\n",
    "    id: \"pw-2\",\n",
    "    server: \"playwright\",\n",
    "    name: \"click\",\n",
    "    description: \"Click an element on a webpage\",\n",
    "    tokens: 490,\n",
    "  },\n",
    "  {\n",
    "    id: \"json-1\",\n",
    "    server: \"utils\",\n",
    "    name: \"parse_json\",\n",
    "    description: \"Parse a JSON string into an object\",\n",
    "    tokens: 320,\n",
    "  },\n",
    "];\n",
    "\n",
    "console.log(`Indexed ${allTools.length} tools (sample from 120 total)`);\n",
    "console.log();\n",
    "console.log(\"Tools by server:\");\n",
    "const byServer = allTools.reduce((acc, t) => {\n",
    "  acc[t.server] = (acc[t.server] || 0) + 1;\n",
    "  return acc;\n",
    "}, {} as Record<string, number>);\n",
    "for (const [server, count] of Object.entries(byServer)) {\n",
    "  console.log(`  ${server}: ${count} tools`);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"read config file and create github issue\"\n",
      "\n",
      "Search Results:\n",
      "==================================================\n",
      "  100% â”‚ github:create_issue\n",
      "      â””â”€ Create a new issue in a GitHub repository\n",
      "  100% â”‚ filesystem:read_file\n",
      "      â””â”€ Read contents of a file from the filesystem\n",
      "  80% â”‚ github:create_pr\n",
      "      â””â”€ Create a pull request\n",
      "  80% â”‚ filesystem:write_file\n",
      "      â””â”€ Write content to a file\n",
      "  60% â”‚ filesystem:list_directory\n",
      "      â””â”€ List files and folders in a directory\n"
     ]
    }
   ],
   "source": [
    "// Simulate semantic search (keyword-based approximation for demo)\n",
    "function searchTools(query: string, topK: number = 5): { tool: Tool; score: number }[] {\n",
    "  const queryWords = query.toLowerCase().split(/\\s+/);\n",
    "\n",
    "  const scored = allTools.map((tool) => {\n",
    "    const text = `${tool.name} ${tool.description}`.toLowerCase();\n",
    "    let score = 0;\n",
    "\n",
    "    for (const word of queryWords) {\n",
    "      if (text.includes(word)) score += 0.3;\n",
    "      if (tool.name.toLowerCase().includes(word)) score += 0.5;\n",
    "    }\n",
    "\n",
    "    // Boost exact matches\n",
    "    if (text.includes(query.toLowerCase())) score += 0.8;\n",
    "\n",
    "    return { tool, score: Math.min(score, 1.0) };\n",
    "  });\n",
    "\n",
    "  return scored\n",
    "    .filter((r) => r.score > 0.2)\n",
    "    .sort((a, b) => b.score - a.score)\n",
    "    .slice(0, topK);\n",
    "}\n",
    "\n",
    "// Test search\n",
    "const query = \"read config file and create github issue\";\n",
    "console.log(`Query: \"${query}\"\\n`);\n",
    "\n",
    "const results = searchTools(query, 5);\n",
    "console.log(\"Search Results:\\n\" + \"=\".repeat(50));\n",
    "for (const { tool, score } of results) {\n",
    "  console.log(`  ${(score * 100).toFixed(0)}% â”‚ ${tool.server}:${tool.name}`);\n",
    "  console.log(`      â””â”€ ${tool.description}`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Measuring the Savings\n",
    "\n",
    "Now let's compare context usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Usage Comparison\n",
      "==================================================\n",
      "\n",
      "TRADITIONAL (load all tools):\n",
      "  Tools loaded:    120\n",
      "  Tokens used:     81,600\n",
      "  Context %:       40.8%\n",
      "\n",
      "ON-DEMAND (vector search):\n",
      "  Tools loaded:    5\n",
      "  Tokens used:     3,220\n",
      "  Context %:       1.61%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸŽ‰ SAVINGS: 96% reduction (25x less context)\n"
     ]
    }
   ],
   "source": [
    "// Calculate context savings\n",
    "const CONTEXT_WINDOW = 200_000;\n",
    "\n",
    "// Traditional approach: load ALL tools\n",
    "const totalAllTools = 120;\n",
    "const avgTokensPerTool = 680;\n",
    "const traditionalTokens = totalAllTools * avgTokensPerTool;\n",
    "const traditionalPct = traditionalTokens / CONTEXT_WINDOW * 100;\n",
    "\n",
    "// On-demand approach: load only matched tools\n",
    "const matchedTools = results.map((r) => r.tool);\n",
    "const onDemandTokens = matchedTools.reduce((sum, t) => sum + t.tokens, 0);\n",
    "const onDemandPct = onDemandTokens / CONTEXT_WINDOW * 100;\n",
    "\n",
    "console.log(\"Context Usage Comparison\\n\" + \"=\".repeat(50));\n",
    "console.log();\n",
    "console.log(\"TRADITIONAL (load all tools):\");\n",
    "console.log(`  Tools loaded:    ${totalAllTools}`);\n",
    "console.log(`  Tokens used:     ${traditionalTokens.toLocaleString()}`);\n",
    "console.log(`  Context %:       ${traditionalPct.toFixed(1)}%`);\n",
    "console.log();\n",
    "console.log(\"ON-DEMAND (vector search):\");\n",
    "console.log(`  Tools loaded:    ${matchedTools.length}`);\n",
    "console.log(`  Tokens used:     ${onDemandTokens.toLocaleString()}`);\n",
    "console.log(`  Context %:       ${onDemandPct.toFixed(2)}%`);\n",
    "console.log();\n",
    "console.log(\"â”€\".repeat(50));\n",
    "const savings = ((traditionalTokens - onDemandTokens) / traditionalTokens * 100).toFixed(0);\n",
    "const reduction = (traditionalPct / onDemandPct).toFixed(0);\n",
    "console.log(`ðŸŽ‰ SAVINGS: ${savings}% reduction (${reduction}x less context)`);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Real Implementation\n",
    "\n",
    "Let's use the actual Casys MCP Gateway search functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gateway not running - using simulation above.\n",
      "To test with real search, start the gateway with: deno task dev\n"
     ]
    }
   ],
   "source": [
    "// Use the real MCP tool for semantic search\n",
    "// This requires the gateway to be running\n",
    "\n",
    "try {\n",
    "  const response = await fetch(\"http://localhost:3000/mcp\", {\n",
    "    method: \"POST\",\n",
    "    headers: { \"Content-Type\": \"application/json\" },\n",
    "    body: JSON.stringify({\n",
    "      jsonrpc: \"2.0\",\n",
    "      id: 1,\n",
    "      method: \"tools/call\",\n",
    "      params: {\n",
    "        name: \"pml_search_tools\",\n",
    "        arguments: {\n",
    "          query: \"take a screenshot of a webpage\",\n",
    "          limit: 5,\n",
    "          include_related: true,\n",
    "        },\n",
    "      },\n",
    "    }),\n",
    "  });\n",
    "\n",
    "  const result = await response.json();\n",
    "  console.log(\"Real Search Results:\");\n",
    "  console.log(JSON.stringify(result, null, 2));\n",
    "} catch (e) {\n",
    "  console.log(\"Gateway not running - using simulation above.\");\n",
    "  console.log(\"To test with real search, start the gateway with: deno task dev\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## The Technical Stack\n\nCasys MCP Gateway uses:\n\n| Component  | Technology             | Purpose                          |\n| ---------- | ---------------------- | -------------------------------- |\n| Embeddings | BGE-M3 (Xenova/bge-m3) | Convert text to 1024-dim vectors |\n| Vector DB  | PGlite + pgvector      | Store and search embeddings      |\n| Similarity | Cosine distance        | Find closest matches             |\n| Caching    | In-memory LRU          | Avoid re-embedding queries       |\n\n### Why BGE-M3?\n\n- **Local**: Runs on your machine, no API calls\n- **Multilingual**: Supports 100+ languages\n- **Quality**: State-of-the-art retrieval performance\n- **Free**: No token costs"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "Before moving on:\n",
    "\n",
    "1. **What is a vector embedding?**\n",
    "   - An array of numbers that captures the semantic meaning of text\n",
    "\n",
    "2. **How does on-demand loading save context?**\n",
    "   - Instead of loading all 120 tools, we search and load only 3-5 relevant ones\n",
    "\n",
    "3. **What's the typical context reduction?**\n",
    "   - From 30-50% down to <5% (often <1%)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [03-dag-execution.ipynb](./03-dag-execution.ipynb) - Parallelize workflows for 5x speedup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}